{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12ca0994",
   "metadata": {},
   "source": [
    "## Evaluation Metrics for Classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054da721",
   "metadata": {},
   "source": [
    "### Confusion Matrix: A confusion matrix is a table that summarizes the performance of a classifier by showing the number of true positive, true negative, false positive, and false negative predictions. It is useful to understand the strengths and weaknesses of a classifier, and also provides information about the distribution of errors made by the classifier:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f914751",
   "metadata": {},
   "source": [
    "#### Accuracy: This metric is defined as the ratio of correctly predicted instances to the total number of instances in the dataset. It measures how often the classifier is correct. However, it can be misleading in imbalanced datasets, as it may give high accuracy scores even if the classifier is not able to accurately predict the minority class. Based on the confusion matrix shows, the accuracy is computed as:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e7b248ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.95\n"
     ]
    }
   ],
   "source": [
    "TP, TN, FP, FN = 4, 91, 1, 4\n",
    "accuracy = (TP + TN)/(TP + TN + FP + FN)\n",
    "print(accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8794c22",
   "metadata": {},
   "source": [
    "#### Precision: Precision is the ratio of correctly predicted positive instances to the total number of instances predicted as positive. It is a measure of the classifier's ability to correctly identify positive instances and avoid false positives.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6324dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.89\n"
     ]
    }
   ],
   "source": [
    "TP = 114\n",
    "FP = 14\n",
    "precision = TP / (TP + FP)\n",
    "print(f\"precision: {precision:4.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a980b53",
   "metadata": {},
   "source": [
    "#### Recall (or Sensitivity): Recall is the ratio of correctly predicted positive instances to the total number of actual positive instances. It is a measure of the classifier's ability to detect all positive instances.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7eb5a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall: 0.97\n"
     ]
    }
   ],
   "source": [
    "recall = TP / (TP + FN)\n",
    "print(f\"recall: {recall:4.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e7301b",
   "metadata": {},
   "source": [
    "#### F1 Score: The F1 Score is the harmonic mean of precision and recall. It balances precision and recall and gives an overall performance score for the classifier.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb7aa552",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9268292682926829"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision = TP / (TP + FP)\n",
    "accuracy = (TP + TN)/(TP + TN + FP + FN)\n",
    "recall = TP / (TP + FN)\n",
    "f1_score = 2 * precision * recall / (precision + recall)\n",
    "f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c096bb96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6\n",
      "Precision:  0.6\n",
      "Recall:  0.6\n",
      "F1 Score:  0.6\n",
      "Confusion Matrix: \n",
      " [[3 2]\n",
      " [2 3]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# True labels of the data\n",
    "y_true = [0, 1, 0, 1, 1, 0, 1, 1, 0, 0]\n",
    "\n",
    "# Predicted labels of the data\n",
    "y_pred = [0, 1, 0, 1, 0, 1, 1, 0, 1, 0]\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "# Calculate precision\n",
    "precision = precision_score(y_true, y_pred)\n",
    "print(\"Precision: \", precision)\n",
    "\n",
    "# Calculate recall\n",
    "recall = recall_score(y_true, y_pred)\n",
    "print(\"Recall: \", recall)\n",
    "\n",
    "# Calculate F1 Score\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "print(\"F1 Score: \", f1)\n",
    "\n",
    "# Calculate Confusion Matrix\n",
    "conf_mat = confusion_matrix(y_true, y_pred)\n",
    "print(\"Confusion Matrix: \\n\", conf_mat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad705cc",
   "metadata": {},
   "source": [
    "### Fit a logistic regression model using the given input and output patterns X and Y, respectively. Once fitted, determine the precision of this fitted logistic regression model using the test dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e0d977",
   "metadata": {},
   "source": [
    "#### X_train = [[4,2,1],[3,4,6],[5,6,7],[8,9,7]]\n",
    "#### y_train = [1,2,1,2]\n",
    "#### X_test = [[4,3,1],[2,4,3],[5,6,1],[5,9,9]]\n",
    "#### y_test = [1,2,2,2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eaa25511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "X_train = [[4,2,1],[3,4,6],[5,6,7],[8,9,7]]\n",
    "y_train = [1,2,1,2]\n",
    "X_test = [[4,3,1],[2,4,3],[5,6,1],[5,9,9]]\n",
    "y_test = [1,2,2,2]\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_score\n",
    "# Train the logistic regression model\n",
    "log_reg = LogisticRegression(random_state=0)\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions with the logistic regression model\n",
    "y_pred_log_reg = log_reg.predict(X_test)\n",
    "\n",
    "# Calculate the evaluation metrics for logistic regression\n",
    "prec_log_reg = precision_score(y_test, y_pred_log_reg, average=\"weighted\")\n",
    "\n",
    "# Print the evaluation metrics for logistic regression\n",
    "print(\"Precision: \", prec_log_reg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc81abd7",
   "metadata": {},
   "source": [
    "#### Fit a Decision Tree model using the input patterns X and the corresponding output patterns Y. After fitting, evaluate the recall of the fitted Decision Tree model using the test dataset.\n",
    "\n",
    "X_train = [[4,2,1],[3,4,6],[5,6,7],[8,9,7]]\n",
    "\n",
    "y_train = [1,2,1,2]\n",
    "\n",
    "X_test = [[4,3,1],[2,4,3],[5,6,1],[5,9,9]]\n",
    "\n",
    "y_test = [1,2,2,2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "74e75fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall:  0.75\n"
     ]
    }
   ],
   "source": [
    "X_train = [[4,2,1],[3,4,6],[5,6,7],[8,9,7]]\n",
    "\n",
    "y_train = [1,2,1,2]\n",
    "\n",
    "X_test = [[4,3,1],[2,4,3],[5,6,1],[5,9,9]]\n",
    "\n",
    "y_test = [1,2,2,2]\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import  recall_score\n",
    "\n",
    "# Train the decision tree model\n",
    "dt = DecisionTreeClassifier(random_state=0)\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions with the decision tree model\n",
    "y_pred_dt = dt.predict(X_test)\n",
    "\n",
    "# Calculate the evaluation metrics for decision tree\n",
    "\n",
    "\n",
    "rec_dt = recall_score(y_test, y_pred_dt, average=\"weighted\")\n",
    "\n",
    "# Print the evaluation metrics for logistic regression\n",
    "print(\"Recall: \", rec_dt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea819f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
